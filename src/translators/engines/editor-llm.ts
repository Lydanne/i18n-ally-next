import type { TranslateOptions, TranslateResult } from './base'
import * as vscode from 'vscode'
import { Config } from '~/core'
import { Log } from '~/utils'
import TranslateEngine from './base'

/** IDE ç¯å¢ƒç±»å‹ */
type IDEType = 'cursor' | 'windsurf' | 'vscode'

/** æ‰¹é‡ç¿»è¯‘è¯·æ±‚é¡¹ */
interface BatchTranslateItem {
  readonly key: string
  readonly text: string
}

/** æ‰¹é‡ç¿»è¯‘ç»“æœ */
type BatchTranslateResult = Record<string, string>

/** æ£€æµ‹å½“å‰ IDE ç¯å¢ƒ */
export function detectIDEType(appName: string): IDEType {
  const name = appName.toLowerCase()
  if (name.includes('cursor'))
    return 'cursor'
  if (name.includes('windsurf'))
    return 'windsurf'
  return 'vscode'
}

/** æ ¹æ® IDE ç±»å‹è·å– vendor */
export function getVendorByIDE(ide: IDEType): string | undefined {
  switch (ide) {
    case 'cursor':
      return 'copilot'
    case 'windsurf':
      return 'copilot'
    case 'vscode':
      return 'copilot'
  }
}

/**
 * è§£ææ‰¹é‡ç¿»è¯‘çš„ JSON å“åº”ï¼ˆçº¯å‡½æ•°ï¼Œæ–¹ä¾¿æµ‹è¯•ï¼‰
 */
export function parseBatchResponse(
  raw: string,
  items: { readonly key: string, readonly text: string }[],
): Record<string, string> {
  try {
    const jsonMatch = raw.match(/\{[\s\S]*\}/)
    if (!jsonMatch)
      throw new Error('No JSON found in response')
    const parsed = JSON.parse(jsonMatch[0]) as Record<string, string>
    return parsed
  }
  catch {
    const result: Record<string, string> = {}
    const lines = raw.split('\n').filter(l => l.trim())
    items.forEach((item, idx) => {
      if (idx < lines.length)
        result[item.key] = lines[idx].replace(/^["']|["']$/g, '').trim()
    })
    return result
  }
}

/** æ¯æ‰¹ç¿»è¯‘çš„æœ€å¤§æ¡æ•° */
const BATCH_SIZE = 20

/**
 * ç¼–è¾‘å™¨å†…ç½®å¤§æ¨¡å‹ç¿»è¯‘å¼•æ“
 * è‡ªåŠ¨æ£€æµ‹ Cursor/Windsurf/VSCode ç¯å¢ƒï¼Œè°ƒç”¨ç¼–è¾‘å™¨å†…ç½® LLM è¿›è¡Œç¿»è¯‘
 */
export default class EditorLLMTranslate extends TranslateEngine {
  private cachedModel: vscode.LanguageModelChat | undefined
  private ideType: IDEType = detectIDEType(vscode.env.appName)

  /**
   * è·å–å¯ç”¨çš„è¯­è¨€æ¨¡å‹
   */
  private async getModel(): Promise<vscode.LanguageModelChat> {
    if (this.cachedModel)
      return this.cachedModel
    if (!vscode.lm?.selectChatModels)
      throw new Error('Language Model API is not available in this editor version')
    const preferredModel = Config.editorLLMModel
    if (preferredModel) {
      const matched = await vscode.lm.selectChatModels({ id: preferredModel })
      if (matched.length) {
        this.cachedModel = matched[0]
        Log.info(`ğŸ¤– IDE: ${this.ideType}, Model (configured): ${matched[0].name} (${matched[0].id})`)
        return this.cachedModel
      }
      Log.info(`ğŸ¤– Configured model "${preferredModel}" not found, falling back to auto-select`)
    }
    const vendor = getVendorByIDE(this.ideType)
    const selector: vscode.LanguageModelChatSelector = vendor ? { vendor } : {}
    const models = await vscode.lm.selectChatModels(selector)
    if (!models.length) {
      const allModels = await vscode.lm.selectChatModels()
      if (allModels.length) {
        this.cachedModel = allModels[0]
        Log.info(`ğŸ¤– Using fallback model: ${allModels[0].name} (${allModels[0].id})`)
        return this.cachedModel
      }
      throw new Error(`No language model available in ${this.ideType}. Please ensure Copilot or AI extension is installed.`)
    }
    this.cachedModel = models[0]
    Log.info(`ğŸ¤– IDE: ${this.ideType}, Model: ${models[0].name} (${models[0].id})`)
    return this.cachedModel
  }

  /**
   * æ„å»ºç¿»è¯‘ prompt
   */
  private buildPrompt(text: string, from: string, to: string): vscode.LanguageModelChatMessage[] {
    return [
      vscode.LanguageModelChatMessage.User(
        `You are a professional i18n translation engine. Translate the following text from "${from}" to "${to}". `
        + `Rules:\n`
        + `1. Return ONLY the translated text, no explanation, no quotes.\n`
        + `2. Keep placeholders like {0}, {name}, {{variable}}, $t(key) unchanged.\n`
        + `3. Keep HTML tags unchanged.\n`
        + `4. Maintain the same tone and style.\n\n`
        + `Text to translate:\n${text}`,
      ),
    ]
  }

  /**
   * æ„å»ºæ‰¹é‡ç¿»è¯‘ prompt
   */
  private buildBatchPrompt(items: BatchTranslateItem[], from: string, to: string): vscode.LanguageModelChatMessage[] {
    const jsonInput = JSON.stringify(
      Object.fromEntries(items.map(i => [i.key, i.text])),
      null,
      2,
    )
    return [
      vscode.LanguageModelChatMessage.User(
        `You are a professional i18n translation engine. Translate all values in the following JSON from "${from}" to "${to}". `
        + `Rules:\n`
        + `1. Return ONLY valid JSON with the same keys and translated values.\n`
        + `2. Keep placeholders like {0}, {name}, {{variable}}, $t(key) unchanged.\n`
        + `3. Keep HTML tags unchanged.\n`
        + `4. Do NOT add any explanation or markdown formatting.\n\n`
        + `JSON to translate:\n${jsonInput}`,
      ),
    ]
  }

  /**
   * å‘é€è¯·æ±‚å¹¶æ”¶é›†å®Œæ•´å“åº”
   */
  private async sendRequest(
    model: vscode.LanguageModelChat,
    messages: vscode.LanguageModelChatMessage[],
  ): Promise<string> {
    const tokenSource = new vscode.CancellationTokenSource()
    const response = await model.sendRequest(messages, {}, tokenSource.token)
    let result = ''
    for await (const fragment of response.text)
      result += fragment
    return result.trim()
  }

  /**
   * å•æ¡ç¿»è¯‘ï¼ˆå®ç° TranslateEngine æ¥å£ï¼‰
   */
  async translate(options: TranslateOptions): Promise<TranslateResult> {
    const { text, from = 'auto', to = 'auto' } = options
    const model = await this.getModel()
    const messages = this.buildPrompt(text, from, to)
    const translatedText = await this.sendRequest(model, messages)
    return {
      text,
      to,
      from,
      response: { model: model.name, ide: this.ideType },
      result: translatedText ? [translatedText] : undefined,
      linkToResult: '',
    }
  }

  /**
   * æ‰¹é‡ç¿»è¯‘å¤šæ¡æ–‡æ¡ˆï¼ˆä¸€æ¬¡è¯·æ±‚ï¼‰
   */
  async translateBatch(
    items: BatchTranslateItem[],
    from: string,
    to: string,
  ): Promise<BatchTranslateResult> {
    if (!items.length)
      return {}
    const model = await this.getModel()
    const allResults: BatchTranslateResult = {}
    const chunks: BatchTranslateItem[][] = []
    for (let i = 0; i < items.length; i += BATCH_SIZE)
      chunks.push(items.slice(i, i + BATCH_SIZE))
    const chunkResults = await Promise.all(
      chunks.map(async (chunk) => {
        const messages = this.buildBatchPrompt(chunk, from, to)
        const raw = await this.sendRequest(model, messages)
        return this.parseBatchResponse(raw, chunk)
      }),
    )
    for (const chunkResult of chunkResults)
      Object.assign(allResults, chunkResult)
    return allResults
  }

  /**
   * è§£ææ‰¹é‡ç¿»è¯‘çš„ JSON å“åº”
   */
  private parseBatchResponse(raw: string, items: BatchTranslateItem[]): BatchTranslateResult {
    return parseBatchResponse(raw, items)
  }

  /**
   * æ£€æŸ¥ LLM API æ˜¯å¦å¯ç”¨
   */
  static async isAvailable(): Promise<boolean> {
    try {
      if (!vscode.lm?.selectChatModels)
        return false
      const models = await vscode.lm.selectChatModels()
      return models.length > 0
    }
    catch {
      return false
    }
  }
}
